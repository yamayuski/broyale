# ECS におけるネットワーク設計（サーバ・クライアント方式）

このドキュメントは、Entity Component
System（ECS）を用いたゲームでのサーバ・クライアント（クライアント/サーバ）方式ネットワーク設計を詳述します。ECS
固有の注意点（コンポーネント単位の複製、エンティティライフサイクル、所有権など）に焦点を当て、実践的なアーキテクチャ、通信フォーマット、同期戦略、最適化技術、セキュリティ、デバッグ方法をまとめます。

## 目的と高レベル方針

- サーバを正（authoritative）とし、ゲーム状態の一貫性を保つ（推奨）。クライアントは入力を送信し、補完（prediction）・補正（reconciliation）を行う。
- 帯域制約と遅延を考慮し、必要な情報のみを効率的に送受信する（差分、興味管理、優先度）。
- ECS
  の設計をネットワーク要件に合わせて調整：どのコンポーネントを複製するか、誰が所有するかを明確化する。

## 用語定義（簡潔）

- Snapshot：ある時点の状態を表すパケット（通常は差分で送る）
- Delta：前回のスナップショットからの差分
- Ghost（または Replica）：クライアント側に複製されたエンティティ
- Ownership：どのノード（サーバ/クライアント）がそのエンティティ／コンポーネントの正当な変更権を持つか
- RPC /
  Command：状態更新のための操作要求（信頼できる/順序保証ありで送ることが多い）

## 基本アーキテクチャ（推奨）

- サーバ（Authoritative World）
  - 実ゲームロジックと最終状態を管理。
  - 入力（Commands）を受け取り処理して状態を更新し、クライアントへスナップショットを配信。
  - チート防止ルールや整合性検査を実行。

- クライアント（Local World）
  - プレイヤー入力をローカルに適用して即時フィードバック（予測）。
  - サーバからのスナップショットを受け取り、ローカル状態を補正（reconcile）。
  - レンダリング専用コンポーネント（visual-only）やローカル専用の一時コンポーネントを持てる。

- メッセージ種類
  - Input / Command（クライアント→サーバ、通常 reliable/unordered 或いは
    reliable/ordered）
  - Snapshot（サーバ→クライアント、主に UDP で不定期/定期に送る、unreliable
    で良いが sequence を付ける）
  - RPC / Event（重要なゲームイベント、通常 reliable）
  - Entity create/destroy（明示的メッセージ、reliable）

## エンティティ識別と ID 管理

- ランタイム ID（短期）と永続 ID（必要時）を区別する。
- サーバ発行の一意な ID を使う（クライアント生成の ID はマッピングを介す）。
- Spawn/Despawn:
  - サーバが spawn メッセージを送り、クライアントはそれを受けて ghost
    を生成する。
  - Despawn は reliable にし、クライアントでリソース解放を行わせる。
- ID 再利用は慎重に：一定期間は再利用しない（stale reference を避ける）。

## コンポーネント単位の複製設計

- 各コンポーネントに対して「ネットワークレプリケーションの方針」を定義する：
  - Replicated full：毎スナップショットで完全データを送る
  - Replicated delta：前回値から差分のみを送る（数値フィールドの XOR/整数差等）
  - Event-only：状態ではなくイベントで送る（例：DamageEvent）
  - Client-own /
    Authority-only：クライアントでのみ意味があるコンポーネント（例：InputBuffer）は送らない
  - Visual-only：クライアントローカルでのみ保持（サーバは不要）
- コンポーネントマスクを用いてどのコンポーネントを送るかバイナリに示すと効率的。

## レプリケーションモデル（状態 vs イベント）

- 状態同期（State Sync）
  - 定期的に snapshot を送り、クライアントは受け取り時点の状態を適用。
  - 良い点：最終的整合性が明確。
  - 課題：帯域と遅延。差分圧縮が必要。

- イベント同期（Event Sync）
  - 重要イベント（攻撃、発生）だけ送る。クライアントはイベントを再生して状態を導出。
  - 良い点：帯域効率が良い。決定的処理で同じ結果が得られるなら有効。
  - 課題：決定性を保つ必要（lockstep や完全奨励は難しい）。

- ハイブリッド：大量の位置/物理は状態同期（差分）、プレイヤーアクションはイベントで送るのが一般的。

## スナップショット設計と差分圧縮

- スナップショット内容
  - フレーム番号（tick）、シーケンス番号
  - Spawn/Despawn のリスト
  - 各エンティティのヘッダ（id、component mask）
  - 各コンポーネントのデータ（差分 or full）
- 差分戦略
  - フィールド単位の差分：前回値と異なるフィールドだけ送る
  - ビットパック：bool フラグ群をビットで詰める
  - 圧縮アルゴリズム：zlib/deflate, LZ4 などを適用（リアルタイム性能を考慮）
  - シーケンス/ACK を用いて古い基準スナップショットを参照した差分を扱う
- 送信頻度
  - 低頻度（1〜10Hz）：重要な状態（HP, status）
  - 高頻度（10〜60Hz）：位置や速度（必要に応じて圧縮と間引き）
- 重要／優先度で送信間隔を変える（プレイヤー・近接オブジェクト高頻度、遠方の低頻度）

## 興味管理（Interest Management）と LOD

- エンティティを全クライアントに送るのは非現実的。Area of
  Interest（AoI）を使う。
- 方法
  - 物理的距離ベース（視界半径）
  - 層/チャンネル（同じ部屋、同じマップ）
  - 観測者依存（カメラ方向や視界によるフィルタリング）
- 実装上はサーバがどのクライアントに何を送るかを決定し、各クライアントにパーソナライズされた
  snapshot を送る。
- LOD: 遠いオブジェクトは更新頻度・精度を落とす（整数化、量子化、間引き）

## クライアントサイド予測（Client-side prediction）とサーバ補正

- 目的：入力遅延を隠し、プレイヤーに即時応答を提供する。
- フロー
  1. クライアントは入力を送信すると同時にローカルで適用（予測）。
  2. サーバは input を処理し、結果を含むスナップショットを返す。
  3. クライアントはスナップショットを受け取り、自分のローカル状態と差があれば補正（reconciliation）する。補正は滑らかに（interpolation
     / snap-to）行う。
- 重要点
  - 入力にシーケンス番号を付与して、サーバの応答がどの入力まで反映しているかを特定する。
  - ローカルでの補正は視覚的に目立たないようにスムーズに行う（position snapping
    を瞬時に行うと違和感）。
  - 物理的な再現が必要な場合、クライアントは補正後に未処理の入力を再適用（re-simulate）する。

## 同期時間・ティック（Tick）と時刻管理

- サーバ側 tick を基準にする（deterministic tick が望ましい）。
- 各パケットに tick/timestamp を含める。
- クライアントはサーバタイムを推定（ラウンドトリップ遅延と時刻オフセットの補正）し、補間バッファを使ってレンダリング時間を遅らせ一定の遅延（例
  100ms）で描画すると安定する。
- ジッター対策としてクライアントは受信タイミングにばらつきがあっても動くように
  interpolation buffer を持つ。

## 信頼性・トランスポート層

- UDP が一般的（低遅延）＋独自の信頼性層（重要メッセージのみ reliable）。
- 重要メッセージ（spawn/despawn, RPC）を reliable にする（SR、ACK、再送）。
- 順序保証が必要な場合と不要な場合を区別して扱う（例：入力は順序保証が重要）。
- 再送戦略とフロー制御（帯域超過時にデータを間引く）を考える。

## セキュリティとチート対策

- サーバは必ず入力を検証し、不正な状態変化を許さない（信頼しない設計）。
- 重要な決定（ダメージ計算、当たり判定の最終判断）はサーバで行う。
- クライアントは軽微な予測と視覚効果を行うのみ。
- パケット署名、暗号化（TLS / DTLS）は機密性が必要なら導入する。
- 不正検知：不整合な状態、頻繁なリスポーン、境界外の座標などのルールをサーバでチェック。

## バージョニングと互換性（プロトコル進化）

- プロトコルにバージョン番号を持たせ、互換性のない変更時はハンドシェイクでバージョンを確認。
- コンポーネントの schema
  バージョン管理：新旧クライアントとの互換化を設計に入れる（optional fields,
  default values）。
- サーバ側でデータのマイグレーション/フォールバックを用意。

## ECS 固有の実装上の考慮点

- コンポーネント単位での差分管理が必要。コンポーネントに「ネットワーク設定」を持たせる（replication
  mode, frequency, quantization）。
- データレイアウト（Archetype）の情報を使って効率的にスナップショットを作れる。例：アーキタイプ単位で走査して近接クライアントにまとめて送る。
- 一部のコンポーネントは「サーバのみ」「クライアントのみ」「双方複製可能」のように厳密に分類する。
- コンポーネントの追加/削除（add/remove）は spawn/despawn と同様に reliable
  に送るか、マスクの差分で送る。
- Entity の移行（ownership transfer）：明示的なメッセージで transfer
  を行い、両者が状態を合意する。

## メッセージ／シリアライズの例（概念）

- スナップショット（簡易表現、バイナリ化前の概念）
  - header: { tick: int, seq: int, clientId: int }
  - spawns: [ { id, archetype, components: [ { typeId, data } ] } ]
  - updates: [ { id, compMask, components: [ { typeId, delta } ] } ]
  - despawns: [ id, ... ]
- 差分の具体例
  - Position コンポーネント: full: { x: float32, y: float32 } / delta: {
    dx:int16, dy:int16 }（量子化して送る）
  - Health: only send when changed（イベントとして扱う）
- RPC/Command:
  - { type: "Fire", targetId, seq, payload }

（実際のプロダクションでは、バイナリ形式にしてフィールドをビットパックし、圧縮をかける）

## デバッグ、プロファイリング、開発ワークフロー

- ログ：送受信スナップショットのサマリ（bytes, entities, components）を出す。
- 可視化：クライアントごとの
  AOI、送信頻度、未適用の差分を表示するダッシュボード。
- リプレイ：サーバ受信した inputs
  をログしてローカルで再生できるようにする（デバッグ／検証用）。
- テスト：遅延・パケットロス・ジッターのシュミレーションを組み込み、対処を確認する。

## ベストプラクティスまとめ

- サーバをオーソリティにして、クライアントは予測・補正を行う。
- コンポーネントごとにレプリケーションポリシーを定義する（頻度、差分、重要度）。
- Spawn/Despawn とコンポーネント add/remove は reliable
  で扱う（もしくは確実に反映する差分方式）。
- 興味管理で帯域を節約し、優先度を導入する。
- 差分圧縮と量子化を導入して帯域を最適化する。
- プロトコルのバージョニングを忘れず、互換性を維持する。
- セキュリティはサーバ側で、クライアントは「信頼できない入力源」と考える。

## 付録：チェックリスト（導入時に検討すべき項目）

- [ ] サーバ・クライアントで「どのコンポーネントを複製するか」を一覧化したか
- [ ] エンティティ ID 発行・再利用ポリシーを決めたか
- [ ] スナップショット形式（full / delta / event）を定義したか
- [ ] 帯域予算に応じて更新頻度と優先度を決めたか
- [ ] 興味管理（AoI）戦略を設計したか
- [ ] クライアント予測・補正のフローを明文化したか
- [ ] トランスポート（UDP/TCP/DTLS 等）と信頼性モデルを決めたか
- [ ] プロトコルのバージョニング方針を定めたか
- [ ] デバッグ用のログ・リプレイ機能を用意したか
